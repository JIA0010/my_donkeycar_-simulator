"""
ç”ŸKerasã§ç”»åƒèªè­˜ã‚’å®Ÿè£…ã™ã‚‹ä¾‹
ãƒ†ãƒ¼ãƒ: æ‰‹æ›¸ãæ•°å­—èªè­˜ï¼ˆMNISTï¼‰- æ©Ÿæ¢°å­¦ç¿’ã®"Hello World"

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ç´„60è¡Œã§ã€ç”»åƒèªè­˜ã®åŸºæœ¬ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
Donkeycarã¨æ¯”è¼ƒã—ã¦ã€ã©ã®ãã‚‰ã„ã®ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ã‹ä½“æ„Ÿã—ã¦ãã ã•ã„ã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
# =============================================================================
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")

# MNIST: 60,000æšã®è¨“ç·´ç”»åƒ + 10,000æšã®ãƒ†ã‚¹ãƒˆç”»åƒï¼ˆ28x28ãƒ”ã‚¯ã‚»ãƒ«ï¼‰
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape} (60000æš, 28x28ãƒ”ã‚¯ã‚»ãƒ«)")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape} (10000æš, 28x28ãƒ”ã‚¯ã‚»ãƒ«)")

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
# =============================================================================
print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ã„ã¾ã™...")

# 1. å½¢çŠ¶ã‚’å¤‰æ›´ (28, 28) â†’ (28, 28, 1) â€»ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒã‚’è¿½åŠ 
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32')
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32')

# 2. æ­£è¦åŒ– (0-255 â†’ 0.0-1.0)
X_train = X_train / 255.0
X_test = X_test / 255.0

# 3. ãƒ©ãƒ™ãƒ«ã‚’one-hot encodingã«å¤‰æ›
# ä¾‹: 3 â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

print(f"å‰å‡¦ç†å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
print(f"å‰å‡¦ç†å¾Œã®ãƒ©ãƒ™ãƒ«: {y_train.shape}")

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
# =============================================================================
print("\nğŸ§  CNNãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")

model = keras.Sequential([
    # å…¥åŠ›å±¤ (28x28x1ã®ç”»åƒ)
    layers.Input(shape=(28, 28, 1)),
    
    # ç¬¬1ç•³ã¿è¾¼ã¿å±¤: 32ãƒ•ã‚£ãƒ«ã‚¿ã€3x3ã‚«ãƒ¼ãƒãƒ«
    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    
    # ç¬¬2ç•³ã¿è¾¼ã¿å±¤: 64ãƒ•ã‚£ãƒ«ã‚¿ã€3x3ã‚«ãƒ¼ãƒãƒ«
    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    
    # å¹³å¦åŒ–ï¼ˆ2D â†’ 1Dï¼‰
    layers.Flatten(),
    
    # å…¨çµåˆå±¤
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),  # éå­¦ç¿’é˜²æ­¢
    
    # å‡ºåŠ›å±¤ï¼ˆ10ã‚¯ãƒ©ã‚¹: 0-9ã®æ•°å­—ï¼‰
    layers.Dense(10, activation='softmax')
])

# ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ã‚’è¡¨ç¤º
model.summary()

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
# =============================================================================
print("\nâš™ï¸ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã„ã¾ã™...")

model.compile(
    optimizer='adam',                      # æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    loss='categorical_crossentropy',       # æå¤±é–¢æ•°ï¼ˆå¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
    metrics=['accuracy']                   # è©•ä¾¡æŒ‡æ¨™
)

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—5: å­¦ç¿’ã®å®Ÿè¡Œ
# =============================================================================
print("\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")

history = model.fit(
    X_train, y_train,                      # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    validation_data=(X_test, y_test),      # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    epochs=5,                               # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’5å›å­¦ç¿’ï¼‰
    batch_size=128,                        # ãƒãƒƒãƒã‚µã‚¤ã‚º
    verbose=1                              # é€²æ—è¡¨ç¤º
)

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
# =============================================================================
print("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¦ã„ã¾ã™...")

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy * 100:.2f}%")
print(f"   ãƒ†ã‚¹ãƒˆæå¤±: {test_loss:.4f}")

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—7: äºˆæ¸¬ã®å®Ÿè¡Œ
# =============================================================================
print("\nğŸ”® ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§äºˆæ¸¬ã‚’å®Ÿè¡Œ...")

# æœ€åˆã®5æšã®ç”»åƒã§äºˆæ¸¬
predictions = model.predict(X_test[:5])

for i in range(5):
    predicted_label = np.argmax(predictions[i])
    true_label = np.argmax(y_test[i])
    confidence = predictions[i][predicted_label] * 100
    
    print(f"ç”»åƒ {i+1}: äºˆæ¸¬={predicted_label} (ä¿¡é ¼åº¦: {confidence:.1f}%), æ­£è§£={true_label}")

# =============================================================================
# ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
# =============================================================================
print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™...")
model.save('mnist_cnn_model.h5')
print("âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ 'mnist_cnn_model.h5' ã«ä¿å­˜ã—ã¾ã—ãŸ")

# =============================================================================
# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
# =============================================================================
print("\nğŸ“Š å­¦ç¿’æ›²ç·šã‚’è¡¨ç¤º...")

plt.figure(figsize=(12, 4))

# æå¤±ã®æ¨ç§»
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='è¨“ç·´æå¤±')
plt.plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
plt.title('ãƒ¢ãƒ‡ãƒ«ã®æå¤±')
plt.xlabel('ã‚¨ãƒãƒƒã‚¯')
plt.ylabel('æå¤±')
plt.legend()
plt.grid(True)

# ç²¾åº¦ã®æ¨ç§»
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='è¨“ç·´ç²¾åº¦')
plt.plot(history.history['val_accuracy'], label='æ¤œè¨¼ç²¾åº¦')
plt.title('ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦')
plt.xlabel('ã‚¨ãƒãƒƒã‚¯')
plt.ylabel('ç²¾åº¦')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_history.png')
print("âœ… å­¦ç¿’æ›²ç·šã‚’ 'training_history.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")

# =============================================================================
# çµæœã®ã¾ã¨ã‚
# =============================================================================
print("\n" + "="*70)
print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
print("="*70)
print(f"""
å®Ÿè£…ã—ãŸã“ã¨:
  âœ“ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç† (æ­£è¦åŒ–ã€å½¢çŠ¶å¤‰æ›´)
  âœ“ CNNãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ (Conv2D Ã— 2, Dense Ã— 1)
  âœ“ ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨å­¦ç¿’
  âœ“ ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨äºˆæ¸¬
  âœ“ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

çµæœ:
  ğŸ“Š æœ€çµ‚ç²¾åº¦: {test_accuracy * 100:.2f}%
  ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: mnist_cnn_model.h5
  ğŸ“ˆ å­¦ç¿’æ›²ç·š: training_history.png

ã‚³ãƒ¼ãƒ‰è¡Œæ•°: ç´„170è¡Œï¼ˆã‚³ãƒ¡ãƒ³ãƒˆè¾¼ã¿ï¼‰
å®Ÿè³ªçš„ãªã‚³ãƒ¼ãƒ‰: ç´„60è¡Œ

ã“ã‚ŒãŒç”ŸKerasã§ã®ç”»åƒèªè­˜ã®åŸºæœ¬ã§ã™ï¼
""")

# =============================================================================
# Donkeycarã¨ã®æ¯”è¼ƒ
# =============================================================================
print("""
ã€Donkeycar vs ç”ŸKerasã€‘

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã‚¿ã‚¹ã‚¯              â”‚ Donkeycar     â”‚ ç”ŸKeras (MNIST)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ãƒ‡ãƒ¼ã‚¿åé›†          â”‚ ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œ   â”‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ          â”‚
â”‚ å‰å‡¦ç†              â”‚ è‡ªå‹•          â”‚ æ‰‹å‹•ï¼ˆç´„10è¡Œï¼‰            â”‚
â”‚ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰          â”‚ è‡ªå‹•          â”‚ æ‰‹å‹•ï¼ˆç´„20è¡Œï¼‰            â”‚
â”‚ å­¦ç¿’å®Ÿè¡Œ            â”‚ 1ã‚³ãƒãƒ³ãƒ‰     â”‚ æ‰‹å‹•ï¼ˆç´„10è¡Œï¼‰            â”‚
â”‚ è©•ä¾¡                â”‚ è‡ªå‹•          â”‚ æ‰‹å‹•ï¼ˆç´„5è¡Œï¼‰             â”‚
â”‚ ä¿å­˜                â”‚ è‡ªå‹•          â”‚ æ‰‹å‹•ï¼ˆ1è¡Œï¼‰               â”‚
â”‚                     â”‚              â”‚                          â”‚
â”‚ åˆè¨ˆã‚³ãƒ¼ãƒ‰é‡        â”‚ 0è¡Œ           â”‚ 60-170è¡Œ                 â”‚
â”‚ é›£æ˜“åº¦              â”‚ â˜…â˜†â˜†â˜†â˜†      â”‚ â˜…â˜…â˜…â˜†â˜†                 â”‚
â”‚ å¿…è¦ãªçŸ¥è­˜          â”‚ åŸºæœ¬æ“ä½œ      â”‚ Python, Keras, CNN       â”‚
â”‚ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§      â”‚ ä¸­            â”‚ é«˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

çµè«–:
- ç°¡å˜ã«å§‹ã‚ãŸã„ â†’ Donkeycar
- æ·±ãç†è§£ã—ãŸã„ â†’ ç”ŸKeras
- ä¸¡æ–¹ã‚„ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆï¼ ğŸ‘
""")
